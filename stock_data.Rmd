---
title: "Data Science Challenge"
output: html_notebook
---


```{r}
# Load packages
library(Hmisc)
library(ggplot2)
library(tidyverse)
library(lubridate)
library(data.table)
library(stringr)
library(readr)
# Load data
returns <- read.table("returns_20181228.csv", header=TRUE, sep=",")

```
# Section 1: Stock Market Data  
  
## Data cleaning
### Date  
```{r}
  n = nrow(returns)
  returns$Date <- as_date(returns$Date)
  a <- c(as.numeric(returns[2:n, 1] - returns[1:n-1, 1]), NA) %>%
    table %>% as.data.frame %>% arrange(Freq)
  names(a) <- c("Days between consecutive records", "Freq")
  print(c(min(returns$Date), max(returns$Date)))
  a
```
The first and the last dates seems reasonable.  
The day difference between the consecutive records are 1 or 3 days. This seems to be OK: five days of market is followed with two missing days (weekends). There is a single case when only one day is missing. I would expect more variability in case of real -not simulated- data.  

### Return values
Check if every column with stock return values are numeric
```{r}
select(returns, -Date) %>%
  apply(2, class) %>%
  unique
```
All values are numeric.  
Check the range of return values and NAs
```{r}
  returns_long <- 
    returns %>%
      gather(-Date, key = "stock", value = "return") %>%
      arrange(abs(return))
  summary(returns_long$return)
```
The value set of return values seems to be OK.  
Frequency table of return values
```{r}
count_values <- function(df){
    df %>%
      gather(-Date, key = "stock", value = "return") %>%
      count(return) %>%
      arrange(desc(n))
  }
  head(count_values(returns))
```
The high number of zeros (53921) is suspicious. They might be missing values.
Have a look at a subset of records with zero return values:
```{r}
returns_long %>% arrange(abs(return)) %>% head
```
Zero return values appear on consecutive days from the same stock. They must be missing values. However, this might not be true for every zero. Before treating every zero as NA I check if there are "real" zeros among them. Could we have that many zeros due to rounding error?
```{r}
  returns %>%
    count_values %>%
    arrange(abs(return)) %>%
    head
```

The smallest absolute values after 0 are of 10^-6 order of magnitude.This suggests that the 0s in the data are not the results of rounding, but they stand for missing data.  
Number of zeros per stock:
```{r}
  count_zeros <- 
    returns_long %>%
      filter(return == 0) %>%
      count(stock) %>%
      arrange(n) 
  head(count_zeros)
```
The number of zeros per stock is 7 or higher. This is also in line with the assumption that the zeros are consecutive and indicate missing data.
Check if all zeros are consecutive within a stock
```{r}
  # The value of the function is 1 if all zeros are consecutive within vector x
  distance_vs_frequency <- function(x) {
    index <- which(x == 0)
    (max(index) - min(index) + 1)/length(index)
  } 
  select(returns, count_zeros$stock) %>%
      apply(2, distance_vs_frequency) %>%
      unique()
```
All zeros are consecutive. They must indicate missing values.

Replace all zeros with NA
```{r}
returns_clr1 <- returns
  returns_clr1[returns_clr1 == 0] <- NA
```
### Checking for duplicates
There are no date duplicate, because the difference between consecutive dates were positive in each case. Check if there are any row duplicate disregarding dates
```{r}
  without_date <- select(returns_clr1, -Date)
  nrow(unique(without_date)) == nrow(returns_clr1)
```
There are no row duplicate.  
Remove duplicated columns
```{r}
  returns_clr2 <- 
    without_date %>%
      t %>%
      unique %>%
      t %>%
      as.data.frame() %>%
      cbind(Date = returns$Date)
head(count_values(returns_clr2))
```
After removing duplicated columns, there are still  duplicated values in the dataset. Theoretically, this could happen by chance, but I rather suspect that some of the duplicated columns were not removed due to NAs.  
Remove duplicated columns, NAs are treated as equal when comparing two columns.
```{r}
  df <- returns_clr2
  tdf <- as.data.frame(t(select(returns_clr2, -Date)))
  for (i in 1: nrow(tdf)){
    if (i > nrow(tdf)) break
    if (anyNA(tdf[i, ]) == TRUE){
      tdf_i <- tdf[ , -which(is.na(tdf[i,]))]
      matches <- find.matches(tdf_i[i,], tdf_i, maxmatch = 2)$matches
      if (!is.na(matches[2])){
        tdf[i,] <- pmin(tdf[i, ], tdf[matches[2], ], na.rm = TRUE)
        tdf <- tdf[!(row.names(tdf) %in% matches[2]), ]
      }
    }
  }
  returns_clr <- cbind(Date = returns$Date, as.data.frame(t(tdf)))
  print(c(ncol(returns_clr), ncol(returns)))
```
After removing duplicates 1015 stocks remained from the original 1140.  
Check again the occurrence of values
```{r}
  head(count_values(returns_clr) %>% arrange(desc(n)))
```
No duplicates
  
## Questions  
### Question 1 - Date of shift  
The analysis is based on the direction of price change, the magnitude of change is not considered. This refelects the definitions of momentum and mean reversion given in the question.  
If the price change on day t is the same as it was on t-1 the "boolean autocorrelation" is defined as 1. If the price change on day t is the opposite as it was on day t-1 the "boolean autocorrelation" is defined as -1. I plotted the average of these "boolean autocorrelation" values vs time. (The average was calculated across stocks.)
```{r}
  # dir_orig: data frame telling the direction of change: 1 -> increase, -1 -> decrease
  dir_orig <- select(returns_clr, -Date)
  dir_orig[dir_orig > 0] <- 1
  dir_orig[dir_orig < 0] <- -1
  dir_orig <- cbind(Date = returns_clr$Date, dir_orig)
  dir_t <- dir_orig[2:nrow(dir_orig), ] #direction of change at date t
  dir_tm1 <- dir_orig[1:nrow(dir_orig)-1, ] #direction of change at date t-1
  
  # cor_m: data frame characterising autocorrelation: 1 -> positive autocorrelation, -1 -> negative autocorrelation
  cor_m <- cbind(Date = dir_t$Date, select(dir_t, -1)*select(dir_tm1, -1)) 
  
  # Take the average of cor_m acoross all stock
  df <- as.data.frame(t(select(cor_m, -1)))
  avg <- cbind(Date = cor_m$Date, 
               as.data.frame(map_dbl(df, mean, na.rm = TRUE)),
               as.data.frame(map_dbl(df, function(x){length(na.omit(x))})))
  names(avg) <- c("Date", "avg", "n")
  avg$avg_n <- avg$avg*avg$n
  
  # plot average autocorrelation vs date
  ggplot(avg, aes(x = Date, y = avg)) + 
    geom_point()+
    labs(y = "Average 'boolean autocorrelation'")

```
At the beginning, the average 'boolean autocorrelation' values are around 0.3 indicating a momentum market. Somewhere around 04.1992 The market is shifted to mean reversion. Have a look at the values around this time:
```{r}
df <- filter(avg[, 1:2], Date > as_date("19920401") & Date < as_date("19920430"))
df
```
The date of shift is 1992.04.23.  
Find the time of shift with an algorithm
```{r}
# the cost is defined as -1 * (the difference between the average autocorrelation values during the two time periode)
  cost_df = data.frame(Date = avg$Date, cost = vector("numeric", nrow(avg)))
  for (i in 1:nrow(avg)){
    avg_1 <- avg[1:i-1, ]
    avg_2 <- avg[i:nrow(avg), ]
    cost_df$cost[[i]] <- -1*(sum(avg_1$avg*avg_1$n)/sum(avg_1$n)-sum(avg_2$avg*avg_2$n)/sum(avg_2$n))^2
  }
  #plot the cost function
  ggplot(cost_df, aes(x = Date, y = cost)) + 
    geom_point()
```
Find the location of minimum
```{r}
  cost_min <- min(cost_df$cost, na.rm = TRUE)
  cost_df$Date[cost_df$cost == cost_min][2]
```
  
### Question 2 - Average stock returns during the momentum period  
  
```{r}
  returns_clr_1 <- returns_clr %>% filter(Date < as_date("1992-04-23")) %>% select(-Date)

  returns_clr_1_long <- 
    returns_clr_1 %>%
    gather(key = "stock", value = "return")
  mean(na.omit((returns_clr_1_long$return)))
```

 
### Question 3 - Average stock returns during the mean reversion period  
  
```{r}
  returns_clr_2 <- returns_clr %>% filter(Date >= as_date("1992-04-23")) %>% select(-Date)

  returns_clr_2_long <- 
    returns_clr_2 %>%
    gather(key = "stock", value = "return")
  mean(na.omit((returns_clr_2_long$return)))
```
  
# Section 2: Oklahoma State Spending  
## Data cleaning
Read data
```{r}
df <- cbind.data.frame(read_lines("res_purchase_2014.csv", skip=1))
df <- mutate_all(df, as.character)

# Delete the quotation mark from the beginning of rows 
  delet_first_quote <- function(x){
    if (x %>% substring(1, 1) == "\""){
      x <- x %>% substring(2)
    }
  }
  
  for (i in 1: nrow(df)){
    delet_first_quote(df[i,1])
  }

  names(df) <- read_lines("res_purchase_2014.csv",n_max=1)
  write.csv(as.data.frame(df), file = "res_mod.csv", row.names=FALSE, quote = FALSE)
  
  df <- read.csv("res_mod.csv", header=TRUE, sep=",", quote = "\"", colClasses = "character")
  names(df) <- c("Date_ym", "Agency_nr", "Agency_name", "Cardholder_last", "Cardholder_first", "Description", "Amount", "Vendor", "Date_tr", "Date_post", "MCC")

```
Dates  
```{r}
# The original year-months column
  df$Date_ym <- gsub("[^0-9]", "", df$Date_ym)
  #table(df$Date_ym)
  df$Date_ym[df$Date_ym == "999" | df$Date_ym == "201900"] <- NA
# Date of transaction
  df$Date_tr <- mdy_hm(df$Date_tr)
# Date posted
  df$Date_post <- mdy_hm(df$Date_post)
```
Amounts
```{r}
  df$Amount <- as.numeric(str_extract(df$Amount, "\\-*\\d+\\.*\\d*"))
  summary(df$Amount)
  hist(log(df$Amount))
```
The distribution of amount values is as expected.  

### Question 1 - Total amount of spending
```{r}
sum(df$Amount)
```
### Question 2 - Total amount of spending at WW GRAINGER  
Search for vendor names similar to WW GRAINGER
```{r}
  df$Vendor <- toupper(df$Vendor)

  a <- agrep("WW GRAINGER", df$Vendor, max.distance = 4,  value = TRUE) %>% table %>% as.data.frame
  names(a) <- c("Vendor", "Freq")
  a
  a <- agrep("GRAINGER", df$Vendor, max.distance = 2,  value = TRUE) %>% table %>% as.data.frame
  names(a) <- c("Vendor", "Freq")
```
There is only one record that belongs to WW GRAINGER with an alternative value in the Vendor column: an adjustment record "CLAIM ADJ/WW GRAINGER"
```{r}
  by_vendor <- df %>%
    select(Vendor, Amount) %>%
    group_by(Vendor) %>%
    dplyr::summarize(total = sum(Amount)) %>%
    arrange(desc(total))
  head(by_vendor)
  by_vendor[by_vendor$Vendor == "WW GRAINGER",]$total + by_vendor[by_vendor$Vendor == "CLAIM ADJ/WW GRAINGER",]$total 
```
### Question 3 - Total amount of spending at WM SUPERCENTER 
Search for vendor names similar to WM SUPERCENTER
```{r}
  
  a <- agrep("WM SUPERCENTER", df$Vendor, max.distance = 2,  value = TRUE) %>% 
    table %>% as.data.frame
  names(a) <- c("Vendor", "Freq")
  a

  df %>% subset(Vendor %in% a$Vendor) %>% select(Amount) %>% sum()
```
### Question 4 - Monthly spendings  
```{r}
  sum(is.na(df$Date_post))
  sum(is.na(df$Date_tr))
  sum(is.na(df$Date_ym))
  missing_ym <- subset(df, is.na(df$Date_ym))
  #View(missing_ym)
  table(missing_ym$Agency_name)
```
There are three different date columns in the dataset. The column called Year-month in the original dataset contains 1230 NA. These records seems otherwise normal, their only specialty that they are all from OKLAHOMA STATE UNIVERSITY.  
The date of transaction and post have no NAs, I will use these. 

Number of records per months - Posted date
```{r}
  df %>%
    select(Date_post) %>%
    group_by("year.month_posted" = paste(year(df$Date_post), month(df$Date_post), sep = ".")) %>%
    dplyr::summarize("number of records" = n())
```
    
      
Number of records per months - Transaction date  
```{r}
 df %>%
    select(Date_tr) %>%
    group_by("year.month_transaction" = paste(year(df$Date_tr), month(df$Date_tr), sep = ".")) %>%
    dplyr::summarize("number of records" = n())
```

```{r}
  a <- as.numeric(df$Date_post - df$Date_tr,  units="days") %>% 
    as.data.frame() %>% summary() %>% as.data.frame() %>% select("Freq")
  names(a) <- c("Descriptives of days between transaction date and posted dates")
  a
```
The number of transactions is much smaller in the first three months than in the subsequent months. It's because some of the spendings, that were posted in the beginning of the analysed time periode (07.01.2013) are coming from the previous months. The monthly spendigs before 07.01.2013 are therefore not representative if I use the date of transaction for grouping.  
  
Std dev of monthly spendings  
Transaction date
```{r}
  by_month_tr <- df %>%
    select(Date_tr, Amount) %>%
    subset(Date_tr >= as_date("2013.07.01")) 
  by_month_tr <- by_month_tr%>%
    group_by("year.month_transaction" = paste(year(by_month_tr$Date_tr), month(by_month_tr$Date_tr), sep = "")) %>%
    dplyr::summarize(total = sum(Amount))
  sd(by_month_tr$total)
```
Posted date
```{r}
  by_month_post <- df %>%
    select(Date_post, Amount) %>%
    group_by("year.month_posted" = paste(year(df$Date_post), month(df$Date_post), sep = "")) %>%
    dplyr::summarize(total = sum(Amount))
  sd(by_month_post$total)
```
As it can be expected the std dev of monthly spending is about the same, no matter which date I use. (The difference is about 5%, which is neglectable when estimating the std dev from such few numbers (i.e. 12 moths).) 
  

